{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81859e86",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60af9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import PeftModel, PeftConfig\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2669c707",
   "metadata": {},
   "source": [
    "Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe644de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"microsoft/codebert-base\"\n",
    "BIGVUL_ADAPTER_PATH = \"./model-bigvul-lora\"\n",
    "JULIET_ADAPTER_PATH = \"./model-juliet-lora\"\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23596aa8",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4314d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8838e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adapter(adapter_path, dataset, dataset_name):\n",
    "    print(f\"\\n========================================================\")\n",
    "    print(f\" EVALUATING MODEL: {adapter_path}\")\n",
    "    print(f\" ON DATASET:       {dataset_name}\")\n",
    "    print(f\"========================================================\")\n",
    "    \n",
    "    # 1. Load Tokenizer\n",
    "    # We load the tokenizer saved with the adapter to ensure consistency\n",
    "    try:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(adapter_path)\n",
    "    except:\n",
    "        # Fallback to base tokenizer if adapter didn't save one\n",
    "        print(\"Warning: Could not load tokenizer from adapter path. Using base tokenizer.\")\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "    # 2. Tokenize Dataset\n",
    "    print(f\"Tokenizing {dataset_name}...\")\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"code\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    # Remove raw text column\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([\"code\"])\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "    # 3. Load Model (Base + Adapter)\n",
    "    print(\"Loading Model...\")\n",
    "    # Load the base model (empty/pretrained weights)\n",
    "    base_model = RobertaForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=2)\n",
    "    # Load the trained LoRA adapter on top\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # 4. Run Evaluation\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./temp_eval_output\", \n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            remove_unused_columns=False\n",
    "        ),\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    print(\"Running Prediction...\")\n",
    "    result = trainer.predict(tokenized_dataset)\n",
    "    \n",
    "    print(\"\\n--- RESULTS ---\")\n",
    "    metrics = result.metrics\n",
    "    # Print clearly\n",
    "    print(f\"Accuracy:  {metrics['test_accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['test_precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['test_recall']:.4f}\")\n",
    "    print(f\"F1 Score:  {metrics['test_f1']:.4f}\")\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ad662",
   "metadata": {},
   "source": [
    "Load and Structure Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06bb0b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Juliet Test Set...\n",
      "Restructuring Juliet Test Set...\n"
     ]
    }
   ],
   "source": [
    "# Prepare Big-Vul Test Set\n",
    "bigvul_dataset = load_dataset(\"bstee615/bigvul\", \"default\", split=\"test\")\n",
    "bigvul_dataset = bigvul_dataset.rename_column(\"func_before\", \"code\")\n",
    "bigvul_dataset = bigvul_dataset.rename_column(\"vul\", \"labels\")\n",
    "# Filter to only keep necessary columns\n",
    "bigvul_test = bigvul_dataset.remove_columns([c for c in bigvul_dataset.column_names if c not in ['code', 'labels']])\n",
    "\n",
    "# B. Prepare Juliet Test Set\n",
    "print(\"Loading Juliet Test Set...\")\n",
    "juliet_dataset = load_dataset(\"LorenzH/juliet_test_suite_c_1_3\", \"default\", split=\"test\")\n",
    "\n",
    "print(\"Restructuring Juliet Test Set...\")\n",
    "def restructure_juliet(examples):\n",
    "    new_codes = []\n",
    "    new_labels = []\n",
    "    goods = examples['good']\n",
    "    bads = examples['bad']\n",
    "    for good_code, bad_code in zip(goods, bads):\n",
    "        if good_code:\n",
    "            new_codes.append(good_code)\n",
    "            new_labels.append(0)\n",
    "        if bad_code:\n",
    "            new_codes.append(bad_code)\n",
    "            new_labels.append(1)\n",
    "    return {\"code\": new_codes, \"labels\": new_labels}\n",
    "\n",
    "# Apply restructuring\n",
    "original_cols = juliet_dataset.column_names\n",
    "juliet_test = juliet_dataset.map(restructure_juliet, batched=True, remove_columns=original_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8638e9",
   "metadata": {},
   "source": [
    "Main Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbbd7598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================\n",
      " EVALUATING MODEL: ./model-bigvul-lora\n",
      " ON DATASET:       Juliet (Synthetic)\n",
      "========================================================\n",
      "Tokenizing Juliet (Synthetic)...\n",
      "Loading Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prediction...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dschr\\Documents\\VSCode\\repos\\Secure_SWE_Project\\sec-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTS ---\n",
      "Accuracy:  0.5000\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 Score:  0.0000\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "========================================================\n",
      " EVALUATING MODEL: ./model-juliet-lora\n",
      " ON DATASET:       Big-Vul (Real-World)\n",
      "========================================================\n",
      "Tokenizing Big-Vul (Real-World)...\n",
      "Loading Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prediction...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTS ---\n",
      "Accuracy:  0.9421\n",
      "Precision: 0.0868\n",
      "Recall:    0.0906\n",
      "F1 Score:  0.0887\n",
      "--------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Experiment A: Big-Vul Model -> Juliet Data\n",
    "if os.path.exists(BIGVUL_ADAPTER_PATH):\n",
    "    evaluate_adapter(BIGVUL_ADAPTER_PATH, juliet_test, \"Juliet (Synthetic)\")\n",
    "else:\n",
    "    print(f\"Skipping Big-Vul evaluation: {BIGVUL_ADAPTER_PATH} not found.\")\n",
    "\n",
    "# Experiment B: Juliet Model -> Big-Vul Data\n",
    "if os.path.exists(JULIET_ADAPTER_PATH):\n",
    "    evaluate_adapter(JULIET_ADAPTER_PATH, bigvul_test, \"Big-Vul (Real-World)\")\n",
    "else:\n",
    "    print(f\"Skipping Juliet evaluation: {JULIET_ADAPTER_PATH} not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sec-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
